---
title: "NYC EMS Dispatch Data"
echo: true
---


```{r loadpackages}
#| echo: false
#| output: false
library(arrow)
library(dplyr)
library(tictoc)
library(readxl)
library(stringr)
library(ggplot2)
```

\

## Create Parquet Files

NYC's EMS dispatch data contains 27M records, resulting in a **5.88GB** csv. 

```{r ems-full-size}
#| eval: false 

list.files("data")
# [1] "EMS_Incident_Dispatch_Data_20250119.csv"

list.files("data", full.names = T) |> 
  file.size() |> 
  scales::comma()
# 6,317,252,621 Bytes
# 5.88 GB
```

This *could* fit in my computer's memory, but for demonstration purposes I will pretend this is beyond my RAM (16GB). I will split this dataset into manageable parquet files using `arrow`.   

```{r ems-full-import}
#| eval: false 

tic()
ems_nyc_full <- arrow::open_dataset(
  sources = "data/EMS_Incident_Dispatch_Data_20250119.csv", 
  col_types = schema(),
  format = "csv"
)
toc() # 0.07 sec
```


Ideally, I would create parquet files by year. However, these date columns aren't in good shape for quick grouping. The csv stored 'MM/DD/YYYY HH:MM:SS' variables as strings rather than date-objects for R.  

```{r ems-full-glimpse}
#| eval: false 

tic()
glimpse(ems_nyc_full)
toc() # 40.22 sec elapsed

# FileSystemDataset with 1 csv file
# 27,223,682 rows x 31 columns
# $ CAD_INCIDENT_ID                 <int64> 71262688, 71262787, …
# $ INCIDENT_DATETIME              <string> "05/06/2007 06:21:01…
# $ INITIAL_CALL_TYPE              <string> "STNDBY", "UNC", "CA…
# $ INITIAL_SEVERITY_LEVEL_CODE     <int64> 8, 2, 3, 5, 7, 2, 6,…
# $ FINAL_CALL_TYPE                <string> "STNDBY", "UNC", "CA…
# $ FINAL_SEVERITY_LEVEL_CODE       <int64> 8, 2, 3, 5, 7, 2, 6,…
# $ FIRST_ASSIGNMENT_DATETIME      <string> "", "05/06/2007 06:5…
# $ VALID_DISPATCH_RSPNS_TIME_INDC <string> "N", "Y", "Y", "Y", …
# $ DISPATCH_RESPONSE_SECONDS_QY    <int64> 0, 12, 60, 0, 154, 5…
# $ FIRST_ACTIVATION_DATETIME      <string> "", "05/06/2007 06:5…
# $ FIRST_ON_SCENE_DATETIME        <string> "", "05/06/2007 07:0…
# $ VALID_INCIDENT_RSPNS_TIME_INDC <string> "N", "Y", "N", "Y", …
# $ INCIDENT_RESPONSE_SECONDS_QY    <int64> NA, 391, NA, 0, 262,…
# $ INCIDENT_TRAVEL_TM_SECONDS_QY   <int64> NA, 379, NA, 0, 108,…
# $ FIRST_TO_HOSP_DATETIME         <string> "", "", "", "05/06/2…
# $ FIRST_HOSP_ARRIVAL_DATETIME    <string> "", "", "", "05/06/2…
# $ INCIDENT_CLOSE_DATETIME        <string> "05/06/2007 06:21:01…
# $ HELD_INDICATOR                 <string> "N", "N", "N", "N", …
# $ INCIDENT_DISPOSITION_CODE      <string> "NOTSNT", "90", "87"…
# $ BOROUGH                        <string> "QUEENS", "BRONX", "…
# $ INCIDENT_DISPATCH_AREA         <string> "Q2", "B3", "Q2", "M…
# $ ZIPCODE                         <int64> NA, NA, NA, 10036, 1…
# $ POLICEPRECINCT                  <int64> NA, NA, NA, 14, 47, …
# $ CITYCOUNCILDISTRICT             <int64> NA, NA, NA, 3, 12, 1…
# $ COMMUNITYDISTRICT               <int64> NA, NA, NA, 104, 212…
# $ COMMUNITYSCHOOLDISTRICT         <int64> NA, NA, NA, 2, 11, 2…
# $ CONGRESSIONALDISTRICT           <int64> NA, NA, NA, 10, 16, …
# $ REOPEN_INDICATOR               <string> "N", "N", "N", "N", …
# $ SPECIAL_EVENT_INDICATOR        <string> "N", "N", "N", "N", …
# $ STANDBY_INDICATOR              <string> "Y", "N", "N", "N", …
# $ TRANSFER_INDICATOR             <string> "N", "N", "N", "N", …
```

In lieu of a prepared date variable, I will split EMS incidents by {BOROUGH}.  

```{r ems-choose-group}
#| eval: false 

tic()
ems_nyc_full |> 
  group_by(BOROUGH) |> 
  count(sort = T) |> 
  collect()
toc()
# 13.48 sec elapsed

# A tibble: 6 × 2
# Groups:   BOROUGH [6]
# BOROUGH                        n
# <chr>                      <int>
# 1 BROOKLYN                 7747447
# 2 MANHATTAN                6717310
# 3 BRONX                    6316780
# 4 QUEENS                   5314633
# 5 RICHMOND / STATEN ISLAND 1127300
# 6 UNKNOWN                      212
```


```{r ems-make-parquet}
#| eval: false 

tic()
ems_nyc_full |>
  group_by(BOROUGH) |> 
  arrow::write_dataset(path = "data/ems-nyc", format = "parquet")
toc()
# 55.36 sec elapsed
```

This created sub-folders named after each Borough. Each contains a parquet file, most of which being in the ideal **20MB - 2GB** range.

```{r ems-parquet-folders}
#| eval: false 

list.files("data/ems-nyc")
# [1] "BOROUGH=BRONX"                           
# [2] "BOROUGH=BROOKLYN"                        
# [3] "BOROUGH=MANHATTAN"                       
# [4] "BOROUGH=QUEENS"                          
# [5] "BOROUGH=RICHMOND%20%2F%20STATEN%20ISLAND"
# [6] "BOROUGH=UNKNOWN" 
```


```{r ems-parquet-size}
#| eval: false 

tibble(
  files = list.files("data/ems-nyc", recursive = TRUE),
  mb = file.size(file.path("data/ems-nyc", files)) / 1024^2 
) |> 
  janitor::adorn_totals() |> 
  mutate(mb = scales::comma(mb))

# files                                mb
# BOROUGH=BRONX/part-0.parquet         480
# BOROUGH=BROOKLYN/part-0.parquet      573
# BOROUGH=MANHATTAN/part-0.parquet     506
# BOROUGH=QUEENS/part-0.parquet        416
# BOROUGH=RICHMOND.../part-0.parquet   126
# BOROUGH=UNKNOWN/part-0.parquet       1
# Total                                2,102
```

These compressed files sum to **2.05GB** - a sharp decrease from the original **5.88GB**!

## Import Parquet  

I can once again use `open_dataset()` to establish a connection to my data. In this case, these are now parquet files.  

```{r}
#| eval: false 

ems_nyc_parquet <- open_dataset("data/ems-nyc")
```

Each EMS indicident is assigned a call type, which is the dispatcher's perceived severity of the incident based on caller details. Most of these require an *external source* to interpret.   

```{r}
#| eval: false 

ems_nyc_parquet |> 
  group_by(FINAL_CALL_TYPE) |> 
  count(sort = T) |> 
  collect()

#    FINAL_CALL_TYPE       n
#    <chr>             <int>
#  1 SICK            4049378
#  2 INJURY          3714001
#  3 DIFFBR          1950231
#  4 EDP             1681874
#  5 DRUG            1663994
#  6 UNC             1527166
#  7 UNKNOW          1302306
#  8 CARD            1203100
#  9 ABDPN           1153570
# 10 MVAINJ           869055
```

To do this, I will merge with an Excel data dictionary.  

### Joins 

```{r}
#| eval: false

call_types_raw <- readxl::read_xlsx(
  "data/EMS_incident_dispatch_data_description.xlsx",
  sheet = 3
)
colnames(call_types_raw) <- paste(
  "call", c("code", "description"),
  sep = "_"
)
```


I could convert this metadata to a parquet file. However, it'd be much easier on R's memory to `collect()` my summary EMS tables, *then* tack on the metadata. Additionally, parquet files allow a **limited** set of dplyr verbs to be applied. Collecting the summary tables circumvents this.       

```{r}
#| eval: false 

### Quick but restricting approach

# Convenient but requires more memory
call_types_arrow <- arrow_table(call_types_raw)

# Resulting file is compatible with limited set of dplyr verbs
ems_nyc_join <- ems_nyc_parquet |>
  left_join(call_types_arrow,
            by = c("FINAL_CALL_TYPE" = "call_code")) 
```


```{r}
#| eval: false

### Longer but flexible approach

# Collect summary tables
borough_call_types <- ems_nyc_parquet |> 
  group_by(BOROUGH, FINAL_CALL_TYPE) |> 
  count() |> 
  collect()

# Then, append metadata as normal
# Can apply more functions, such as slice()
borough_call_types_top5 <- borough_call_types |> 
  left_join(call_types_raw,
            by = c("FINAL_CALL_TYPE" = "call_code")) |> 
  arrange(BOROUGH, desc(n)) |> 
  group_by(BOROUGH) |> 
  slice(1:5) |>
  mutate(delete = 1) |> 
  mutate(rank = cumsum(delete),
         call_description = 
           stringr::str_to_title(call_description)) |> 
  select(borough = BOROUGH, 
         rank, 
         call_code = FINAL_CALL_TYPE,
         call_description, 
         calls = n)
```


A quick glance of the summary table, to be used in Analysis: 

```{r}
#| eval: false 
borough_call_types_top5 |> head()

#   borough   rank call_code call_description      calls
#   <chr>    <dbl> <chr>     <chr>                 <int>
# 1 BRONX        1 SICK      Sick                1019300
# 2 BRONX        2 INJURY    Non-Critical Injury  839998
# 3 BRONX        3 DIFFBR    Difficult Breather   513740
# 4 BRONX        4 EDP       Psychiatric Patient  391644
# 5 BRONX        5 ABDPN     Abdominal Pain       327507
# 6 BROOKLYN     1 SICK      Sick                1173287
```



## Analysis  





